{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are three pre-processing steps before feeding the speech/sentences in the algorithom for the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download() # run thi line if you don't have nltk\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer # need for stemm process\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer # need for lemmatization process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph =  \"\"\"Human beings have an innate inner drive to be autonomous, self-determined and connected to one another.\n",
    "                And, when that drive is liberated, people achieve more and live richer lives. \n",
    "                The stars will never align, and the traffic lights of life will never all be green at the same time. \n",
    "                The universe doesn't conspire against you, but it doesn't go out of its way to line up the pins either. \n",
    "                Conditions are never perfect.\n",
    "                'Someday' is a disease that will take your dreams to the grave with you. \n",
    "                Pro and con lists are just as bad. \n",
    "                If it's important to you and you want to do it 'eventually,' just do it and correct course along the way.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization (Step-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Human beings have an innate inner drive to be autonomous, self-determined and connected to one another.', 'And, when that drive is liberated, people achieve more and live richer lives.', 'The stars will never align, and the traffic lights of life will never all be green at the same time.', \"The universe doesn't conspire against you, but it doesn't go out of its way to line up the pins either.\", 'Conditions are never perfect.', \"'Someday' is a disease that will take your dreams to the grave with you.\", 'Pro and con lists are just as bad.', \"If it's important to you and you want to do it 'eventually,' just do it and correct course along the way.\"]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences) # to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Human', 'beings', 'have', 'an', 'innate', 'inner', 'drive', 'to', 'be', 'autonomous', ',', 'self-determined', 'and', 'connected', 'to', 'one', 'another', '.', 'And', ',', 'when', 'that', 'drive', 'is', 'liberated', ',', 'people', 'achieve', 'more', 'and', 'live', 'richer', 'lives', '.', 'The', 'stars', 'will', 'never', 'align', ',', 'and', 'the', 'traffic', 'lights', 'of', 'life', 'will', 'never', 'all', 'be', 'green', 'at', 'the', 'same', 'time', '.', 'The', 'universe', 'does', \"n't\", 'conspire', 'against', 'you', ',', 'but', 'it', 'does', \"n't\", 'go', 'out', 'of', 'its', 'way', 'to', 'line', 'up', 'the', 'pins', 'either', '.', 'Conditions', 'are', 'never', 'perfect', '.', \"'Someday\", \"'\", 'is', 'a', 'disease', 'that', 'will', 'take', 'your', 'dreams', 'to', 'the', 'grave', 'with', 'you', '.', 'Pro', 'and', 'con', 'lists', 'are', 'just', 'as', 'bad', '.', 'If', 'it', \"'s\", 'important', 'to', 'you', 'and', 'you', 'want', 'to', 'do', 'it', \"'eventually\", ',', \"'\", 'just', 'do', 'it', 'and', 'correct', 'course', 'along', 'the', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words) # to see the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming or Lemmatization (Step-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human innat inner drive autonom self determin connect one anoth\n",
      "drive liber peopl achiev live richer life\n",
      "star never align traffic light life never green time\n",
      "univers n conspir n go way line pin either\n",
      "condit never perfect\n",
      "someday diseas take dream grave\n",
      "pro con list bad\n",
      "import want eventu correct cours along way\n"
     ]
    }
   ],
   "source": [
    "#Stemming process to find out the root word \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "corpus = [] # after cleanning the sentences we will put all the sentances into the courpus variable.\n",
    "            #So that in future we can compare our real paragraph and the after clean paragraph or sentance\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) # removing all the punctuation except letters by space \n",
    "    review = review.lower() # lower case all the letter\n",
    "    review = review.split() # So the same as the nltk.word_tokenize() do\n",
    "    #review = nltk.word_tokenize(review)# do the same as the .splite() do\n",
    "    #print(review)\n",
    "    review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))] #Stemming\n",
    "    review = ' '.join(review)\n",
    "    print(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "    \n",
    "\n",
    "# observe the stemming output result very carefully.most of the words don't have any proper meaning.\n",
    "#this is a problem of stemming. This problem can be solve if we use Lemmatization process instead of stemming.\n",
    "#Though stemming process has some good applicaion like spam mail detaction, stemming process take less time compare to Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human innate inner drive autonomous self determined connected one another\n",
      "drive liberated people achieve live richer life\n",
      "star never align traffic light life never green time\n",
      "universe n conspire n go way line pin either\n",
      "condition never perfect\n",
      "someday disease take dream grave\n",
      "pro con list bad\n",
      "important want eventually correct course along way\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization process to find out the root word \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "corpus = [] # after cleanning the sentences we will put all the sentances into the courpus variable.\n",
    "            #So that in future we can compare our real paragraph and the after clean paragraph or sentance\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) # removing all the punctuation except letters by space \n",
    "    review = review.lower()\n",
    "    review = review.split() # So the same as the nltk.word_tokenize() do\n",
    "    #review = nltk.word_tokenize(review)# do the same as the .splite() do\n",
    "    #print(review)\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))] # Lemmatization\n",
    "    review = ' '.join(review)\n",
    "    print(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# observe the lemmatizer output result very carefully.most of the words have proper meaning.\n",
    "#try to compare the two process output \n",
    "#Lemmatization process has some good applicaion like google home,alexa,amazon echo,siri in word to process and understand the language\n",
    "#but it takes longer time than stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words or TF-IDF(Step-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model. Bag of words is nothing but, it creates the matrix for the paragraph \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "# Bag of words has a draw back that is, In the Bag of Words model give same importance to all the words.\n",
    "#To overcome this draw back TF-TDF model is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the TF-IDF model.TF-IDF  is nothing but, it creates the matrix for the paragraph \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
